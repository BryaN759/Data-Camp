{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric data or ... ?\n",
    "#------1-------\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "\n",
    "\n",
    "#-------3------\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())\n",
    "\n",
    "\n",
    "\n",
    "#Summing strings and concatenating numbers\n",
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tire size constraints\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes']  > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Back to the future\n",
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())\n",
    "\n",
    "\n",
    "#Finding duplicates\n",
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
    "\n",
    "\n",
    "\n",
    "#Treating duplicates\n",
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding consistency\n",
    "\n",
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines[\"safety\"].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer- cleanliness because it has as Unacceptable category\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "#Inconsistent categories\n",
    "\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].value_counts())\n",
    "print(airlines['dest_size'].value_counts())\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer- Both 2 and 3\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "###################################################################\n",
    "#Remapping categories\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], \n",
    "                               bins = label_ranges, \n",
    "                               labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "#Removing titles and taking names\n",
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines[\"full_name\"].str.replace(\"Miss\",\"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace('Ms.',\"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "#Keeping it descriptive\n",
    "\n",
    "# Store length of each row in survey_response column\n",
    "resp_length =airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ambiguous dates\n",
    "\n",
    "#Question\n",
    "#Answer-Set them to NA and drop them.\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "#Uniform currencies\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[banking['acct_cur']=='euro', 'acct_amount'] = banking.loc[banking['acct_cur']=='euro', 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[banking['acct_cur']=='euro', 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'\n",
    "\n",
    "############################################################################\n",
    "\n",
    "#Uniform dates\n",
    "\n",
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer-The 21-14-17 is an erroneous and leads to an error\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "#How's our data integrity?\n",
    "\n",
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "#Is this missing at random?\n",
    "\n",
    "#Question\n",
    "#Anser-Missing completely at random.\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "#Missing investors\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking[\"inv_amount\"].isna()]\n",
    "investors = banking[~banking[\"inv_amount\"].isna()]\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer-The inv_amount  is missing only for young customers, since the average age in missing_inversotr  is 22 and the maximum age is 25.\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by = \"age\")\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "#Follow the money\n",
    "\n",
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount']*5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum edit distance\n",
    "\n",
    "#Question\n",
    "#Answer- 2 by substituting 'g' with 'n' and 'n' with 'g'.\n",
    "\n",
    "####################################################################\n",
    "\n",
    "#The cutoff point\n",
    "\n",
    "# Import process from thefuzz\n",
    "from thefuzz import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer-80\n",
    "\n",
    "######################################################################\n",
    "\n",
    "#Remapping categories II\n",
    "\n",
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['cuisine_type'].unique())\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit = len(restaurants))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1]>=80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "for cuisine in categories:\n",
    "    # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "    matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "    # Iterate through the list of matches\n",
    "    for match in matches:\n",
    "        # Check whether the similarity score is greater than or equal to 80\n",
    "        if match[1] >= 80:\n",
    "            # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "            restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "#Pairs of restaurants\n",
    "\n",
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer-Compare between columns, score the comparison, then link the DataFrames.\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "#Similar restaurants\n",
    "\n",
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types\n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types\n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types -\n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "#Question\n",
    "#Answer- 3 because I need to have matches in all my columns\n",
    "\n",
    "###################################################################\n",
    "\n",
    "#Question\n",
    "#Answer-matches.index.get_level_values(0)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "#Linking them together!\n",
    "\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)\n",
    "\n",
    "#######################################################################\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
