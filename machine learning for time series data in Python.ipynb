{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Timeseries kinds and applications**\n",
    "___\n",
    "- data that changes over time\n",
    "    - e.g., atmospheric changes, demographic information, financial data, voice wave forms\n",
    "    - datapoints and timestamps for each data point\n",
    "- in machine learning, changes over time shows useful patterns in machine learning\n",
    "- a machine learning pipeline\n",
    "    - feature extraction\n",
    "    - model fitting\n",
    "    - prediction and validation\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "A list of the average length of each class at the school"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a time series (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows of data\n",
    "print(data.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    symbol  data_values\n",
    "#    0        214.009998\n",
    "#    1        214.379993\n",
    "#    2        210.969995\n",
    "#    3        210.580000\n",
    "#    4        211.980005\n",
    "#################################################\n",
    "\n",
    "# Print the first 5 rows of data2\n",
    "print(data2.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       data_values\n",
    "#    0    -0.006928\n",
    "#    1    -0.007929\n",
    "#    2    -0.008900\n",
    "#    3    -0.009815\n",
    "#    4    -0.010653\n",
    "#################################################\n",
    "\n",
    "# Plot the time series in each dataset\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "data.iloc[:1000].plot(y=\"data_values\", ax=axs[0])\n",
    "data2.iloc[:1000].plot(y=\"data_values\", ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a time series (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the time series in each dataset\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
    "data.iloc[:1000].plot(x=\"time\", y=\"data_values\", ax=axs[0])\n",
    "data2.iloc[:1000].plot(x=\"time\", y=\"data_values\", ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Machine learning basics**\n",
    "___\n",
    "- always begin by looking at your data\n",
    "- scikit-learn data needs to be 2 dimensional\n",
    "    - (samples, features)\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple model: classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows for inspection\n",
    "print(data.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
    "#    50                7.0               3.2                4.7               1.4\n",
    "#    51                6.4               3.2                4.5               1.5\n",
    "#    52                6.9               3.1                4.9               1.5\n",
    "#    53                5.5               2.3                4.0               1.3\n",
    "#    54                6.5               2.8                4.6               1.5\n",
    "#\n",
    "#        target\n",
    "#    50       1\n",
    "#    51       1\n",
    "#    52       1\n",
    "#    53       1\n",
    "#    54       1\n",
    "#################################################\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Construct data for the model\n",
    "X = data[['petal length (cm)', 'petal width (cm)']]\n",
    "y = data[['target']]\n",
    "\n",
    "# Fit the model\n",
    "model = LinearSVC()\n",
    "model.fit(X, y)\n",
    "\n",
    "#################################################\n",
    "#You've successfully fit a classifier to predict flower type!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting using a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create input array\n",
    "X_predict = targets[['petal length (cm)', 'petal width (cm)']]\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(X_predict)\n",
    "print(predictions)\n",
    "\n",
    "# Visualize predictions and actual values\n",
    "plt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n",
    "            c=predictions, cmap=plt.cm.coolwarm)\n",
    "plt.title(\"Predicted class values\")\n",
    "plt.show()\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [2 2 2 1 1 2 2 2 2 1 2 1 1 2 1 1 2 1 2 2]\n",
    "#################################################\n",
    "#Note that the output of your predictions are all integers,\n",
    "#representing that datapoint's predicted class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple model: regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Prepare input and output DataFrames\n",
    "X = housing[['MedHouseVal']]  \n",
    "y = housing[['AveRooms']]     \n",
    "\n",
    "# Fit the model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "#################################################\n",
    "# In regression, the output of your model is a continuous array of\n",
    "#numbers, not class identity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting using a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions with the model using those inputs\n",
    "predictions = model.predict(new_inputs.reshape(-1, 1))\n",
    "\n",
    "# Visualize the inputs and predicted values\n",
    "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
    "plt.xlabel('inputs')\n",
    "plt.ylabel('predictions')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Machine learning and time series data**\n",
    "___\n",
    "- using audio data of heart sounds to detect who has a heart condition\n",
    "- using new york stock exchange data to detect patterns in historical records that allow us to predict the value of companies in the future\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import librosa as lr\n",
    "from glob import glob\n",
    "\n",
    "# List all the wav files in the folder\n",
    "audio_files = glob(data_dir + '/*.wav')\n",
    "\n",
    "# Read in the first audio file, create the time array\n",
    "audio, sfreq = lr.load(audio_files[0])\n",
    "time = np.arange(0, len(audio)) / sfreq\n",
    "\n",
    "# Plot audio over time\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, audio)\n",
    "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the regression data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('prices.csv', index_col=0)\n",
    "\n",
    "# Convert the index of the DataFrame to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "print(data.head())\n",
    "\n",
    "# Loop through each column, plot its values over time\n",
    "fig, ax = plt.subplots()\n",
    "for column in data.columns:\n",
    "    data[column].plot(ax=ax, label=column)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                      AAPL  FB       NFLX          V        XOM\n",
    "#    time\n",
    "#    2010-01-04  214.009998 NaN  53.479999  88.139999  69.150002\n",
    "#    2010-01-05  214.379993 NaN  51.510001  87.129997  69.419998\n",
    "#    2010-01-06  210.969995 NaN  53.319999  85.959999  70.019997\n",
    "#    2010-01-07  210.580000 NaN  52.400001  86.760002  69.800003\n",
    "#    2010-01-08  211.980005 NaN  53.300002  87.000000  69.519997\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Classifying a time series**\n",
    "___\n",
    "- always visualize raw data before fitting models\n",
    "- start with summary statistics\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many repetitions of sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
    "\n",
    "# Calculate the time array\n",
    "time = np.arange(normal.shape[0]) / sfreq\n",
    "\n",
    "# Stack the normal/abnormal audio so you can loop and plot\n",
    "stacked_audio = np.hstack([normal, abnormal]).T\n",
    "\n",
    "# Loop through each audio file / ax object and plot\n",
    "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
    "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
    "    ax.plot(time, iaudio)\n",
    "show_plot_and_make_titles()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariance in time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Average across the audio files of each DataFrame\n",
    "mean_normal = np.mean(normal, axis=1)\n",
    "mean_abnormal = np.mean(abnormal, axis=1)\n",
    "\n",
    "# Plot each average over time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "ax1.plot(time, mean_normal)\n",
    "ax1.set(title=\"Normal Data\")\n",
    "ax2.plot(time, mean_abnormal)\n",
    "ax2.set(title=\"Abnormal Data\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and score them manually\n",
    "predictions = model.predict(X_test)\n",
    "print(sum(predictions == y_test.squeeze()) / len(y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.555555555556\n",
    "#################################################\n",
    "#Note that your predictions didn't do so well. That's because the\n",
    "#features you're using as inputs to the model (raw data) aren't very\n",
    "#good at differentiating classes. Next, you'll explore how to calculate\n",
    "#some more complex features that may improve the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Improving features for classification**\n",
    "___\n",
    "- The auditory envelope\n",
    "    - smooth the data to calculate the auditory envelope\n",
    "    - related to the amount of audio energy present at each moment in time\n",
    "- smoothing over time\n",
    "    - instead of averaging over time, we do a local average\n",
    "    - this is called smoothing your timeseries\n",
    "    - it removes short-term noise, while retaining the general pattern\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the envelope of sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the raw data first\n",
    "audio.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Rectify the audio signal\n",
    "audio_rectified = audio.apply(np.abs)\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Smooth by applying a rolling mean\n",
    "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified_smooth.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating features from the envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate stats\n",
    "means = np.mean(audio_rectified_smooth, axis=0)\n",
    "stds = np.std(audio_rectified_smooth, axis=0)\n",
    "maxs = np.max(audio_rectified_smooth, axis=0)\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.716666666667\n",
    "#################################################\n",
    "#This model is both simpler (only 3 features) and more understandable\n",
    "#(features are simple summary statistics of the data)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative features: The tempogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the tempo of the sounds\n",
    "tempos = []\n",
    "for col, i_audio in audio.items():\n",
    "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# Convert the list to an array so you can manipulate it more easily\n",
    "tempos = np.array(tempos)\n",
    "\n",
    "# Calculate statistics of each tempo\n",
    "tempos_mean = tempos.mean(axis=-1)\n",
    "tempos_std = tempos.std(axis=-1)\n",
    "tempos_max = tempos.max(axis=-1)\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.533333333333\n",
    "#################################################\n",
    "#Note that your predictive power may not have gone up (because this\n",
    "#dataset is quite small), but you now have a more rich feature\n",
    "#representation of audio that your model can use!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**The spectrogram**\n",
    "___\n",
    "- fourier transform\n",
    "    - timeseries data can be described as a combination of quickly-changing and slowly-changing things\n",
    "    - at each moment in time, we can describe the relative presence of fast- and slow-moving components\n",
    "    - this converts a single timeseries into an array that describes the timeseries as a combination of oscillations\n",
    "- short time (st) fft is squared = spectrogram\n",
    "- spectral feature engineering\n",
    "    - each timeseries has a different spectral pattern\n",
    "    - we can calculate these spectral patterns by analyzing the spectrogram to describe where most of the energy is at each moment in time\n",
    "        - **spectral bandwidth**\n",
    "        - **spectral centroids**\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms of heartbeat audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the functions you'll use for the STFT\n",
    "from librosa.core import stft\n",
    "\n",
    "# Prepare the STFT\n",
    "HOP_LENGTH = 2**4\n",
    "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
    "\n",
    "########################################################\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert into decibels\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Compare the raw audio to the spectrogram of the audio\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "axs[0].plot(time, audio)\n",
    "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering spectral features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import librosa as lr\n",
    "\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n",
    "###################################################\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert spectrogram to decibels for visualization\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH, ax=ax)\n",
    "ax.plot(times_spec, centroids)\n",
    "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
    "ax.set_ylim([0, 6000])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining many features in a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through each spectrogram\n",
    "bandwidths = []\n",
    "centroids = []\n",
    "\n",
    "for spec in spectrograms:\n",
    "    # Calculate the mean spectral bandwidth\n",
    "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
    "    # Calculate the mean spectral centroid\n",
    "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
    "    # Collect the values\n",
    "    bandwidths.append(this_mean_bandwidth)\n",
    "    centroids.append(this_mean_centroid)\n",
    "\n",
    "#################################################\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.483333333333\n",
    "#################################################\n",
    "#You calculated many different features of the audio, and combined\n",
    "#each of them under the assumption that they provide independent\n",
    "#information that can be used in classification. You may have noticed\n",
    "#that the accuracy of your models varied a lot when using different\n",
    "#set of features. This chapter was focused on creating new \"features\"\n",
    "#from raw data and not obtaining the best accuracy. To improve the\n",
    "#accuracy, you want to find the right features that provide relevant\n",
    "#information and also build models on much larger data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Predicting data over time**\n",
    "___\n",
    "- correlation between variables often changes over time\n",
    "    - two timeseries that seem correlated at one moment may not remain so over time\n",
    "- Coefficient of Determination (R squared)\n",
    "    - 1 - (error of the model / variance of test data\n",
    "    - \"model accounts for % of variance of the model\"\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the raw values over time\n",
    "prices.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplot with one company per axis\n",
    "prices.plot.scatter(\"EBAY\", \"YHOO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplot with color relating to time\n",
    "prices.plot.scatter('EBAY', 'YHOO', c=prices.index,\n",
    "                    cmap=plt.cm.viridis, colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use stock symbols to extract training data\n",
    "X = all_prices[['EBAY', 'NVDA', 'YHOO']]\n",
    "y = all_prices[['AAPL']]\n",
    "\n",
    "# Fit and score the model with cross-validation\n",
    "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
    "print(scores)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [-6.09050633 -0.3179172  -3.72957284]\n",
    "#################################################\n",
    "#As you can see, fitting a model with raw data doesn't give great results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Split our data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=.8, shuffle=False, random_state=1)\n",
    "\n",
    "# Fit our model and generate predictions\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "score = r2_score(y_test, predictions)\n",
    "print(score)\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Visualize our predictions along with the \"true\" values, and print the score\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(y_test, color='k', lw=3)\n",
    "ax.plot(predictions, color='r', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Advanced time series prediction**\n",
    "___\n",
    "- cleaning messy data\n",
    "    - interpolating missing values\n",
    "    - transforming data to standardize variance\n",
    "    - finding outliers (> 3 sd) in data\n",
    "        - replace outliers with threeshold/median\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing messy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "prices.plot(legend=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count the missing values of each time series\n",
    "missing_values = prices.isna().sum()\n",
    "print(missing_values)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    symbol\n",
    "#    EBAY    273\n",
    "#    NVDA    502\n",
    "#    YHOO    232\n",
    "#    dtype: int64\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a function we'll use to interpolate and plot\n",
    "def interpolate_and_plot(prices, interpolation):\n",
    "\n",
    "    # Create a boolean mask for missing values\n",
    "    missing_values = prices.isna()\n",
    "\n",
    "    # Interpolate the missing values\n",
    "    prices_interp = prices.interpolate(interpolation)\n",
    "\n",
    "    # Plot the results, highlighting the interpolated values in black\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
    "\n",
    "    # Now plot the interpolated values on top in red\n",
    "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
    "    plt.show()\n",
    "\n",
    "###############################################\n",
    "\n",
    "# Interpolate using the latest non-missing value\n",
    "interpolation_type = 'zero'\n",
    "interpolate_and_plot(prices, interpolation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Interpolate linearly\n",
    "interpolation_type = 'linear'\n",
    "interpolate_and_plot(prices, interpolation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Interpolate with a quadratic function\n",
    "interpolation_type = 'quadratic'\n",
    "interpolate_and_plot(prices, interpolation_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Your custom function\n",
    "def percent_change(series):\n",
    "    # Collect all *but* the last value of this window, then the final value\n",
    "    previous_values = series[:-1]\n",
    "    last_value = series[-1]\n",
    "\n",
    "    # Calculate the % difference between the last value and the mean of earlier values\n",
    "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "    return percent_change\n",
    "\n",
    "# Apply your custom function and plot\n",
    "prices_perc = prices.rolling(20).apply(percent_change)\n",
    "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_outliers(series):\n",
    "    # Calculate the absolute difference of each timepoint from the series mean\n",
    "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
    "\n",
    "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
    "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
    "\n",
    "    # Replace these values with the median accross the data\n",
    "    series[this_mask] = np.nanmedian(series)\n",
    "    return series\n",
    "\n",
    "# Apply your preprocessing function to the timeseries and plot the results\n",
    "prices_perc = prices_perc.apply(replace_outliers)\n",
    "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Creating features over time**\n",
    "___\n",
    "- using .aggregate for feature extraction\n",
    "- using .partial() functions in Python\n",
    "- percentiles summarize your data\n",
    "    - percentiles are a useful way to get more fine-grained summaries of your data (as opposed to using np.mean)\n",
    "- calculating \"date-based\" features\n",
    "    - using Pandas .index\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering multiple rolling features at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
    "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "\n",
    "# Define the features you'll calculate for each window\n",
    "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
    "\n",
    "# Calculate these features for your rolling window object\n",
    "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
    "\n",
    "# Plot the results\n",
    "ax = features.loc[:\"2011-01\"].plot()\n",
    "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
    "ax.legend(loc=(1.01, .6))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentiles and partial functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import partial from functools\n",
    "from functools import partial\n",
    "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
    "\n",
    "# Use a list comprehension to create a partial function for each quantile\n",
    "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
    "\n",
    "# Calculate each of these quantiles on the data using a rolling window\n",
    "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
    "\n",
    "# Plot a subset of the result\n",
    "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
    "ax.legend(percentiles, loc=(1.01, .5))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"date\" information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract date features from the data, add them as columns\n",
    "prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
    "prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
    "prices_perc['month_of_year'] = prices_perc.index.month\n",
    "\n",
    "# Print prices_perc\n",
    "print(prices_perc)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    EBAY  day_of_week  week_of_year  month_of_year\n",
    "#    date\n",
    "#    2014-01-02  0.017938            3             1              1\n",
    "#    2014-01-03  0.002268            4             1              1\n",
    "#    2014-01-06 -0.027365            0             2              1\n",
    "#    2014-01-07 -0.006665            1             2              1\n",
    "#    2014-01-08 -0.017206            2             2              1\n",
    "#    2014-01-09 -0.023270            3             2              1\n",
    "#    2014-01-10 -0.022257            4             2              1\n",
    "#\n",
    "#    ...              ...          ...           ...            ...\n",
    "#\n",
    "#    [504 rows x 4 columns]\n",
    "#################################################\n",
    "#This concludes the third chapter. In the next chapter, you will\n",
    "#learn advanced techniques to validate and inspect your time series\n",
    "#models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Creating features from the past**\n",
    "___\n",
    "- The past is useful\n",
    "    - timeseries data almost always have information that is shared between timepoints\n",
    "    - information in the past can help predict what happens in the future\n",
    "    - often the features best suited to predict a timeseries are previous values of the same timeseries\n",
    "- A note on smoothness and auto-correlation\n",
    "    - a common question to ask of a timeseries: how smooth is the data?\n",
    "    - or, how correlated is a timepoint with its neighboring timepoints (called **autocorrelation**)\n",
    "    - the amount of auto-correlation in data will impact your models\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating time-shifted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# These are the \"time lags\"\n",
    "shifts = np.arange(1, 11).astype(int)\n",
    "\n",
    "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
    "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
    "\n",
    "# Convert into a DataFrame for subsequent use\n",
    "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
    "\n",
    "# Plot the first 100 samples of each\n",
    "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
    "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: Auto-regressive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replace missing values with the median for each column\n",
    "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
    "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
    "\n",
    "# Fit the model\n",
    "model = Ridge()\n",
    "model.fit(X, y)\n",
    "\n",
    "#################################################\n",
    "#You've filled in the missing values with the median so that it\n",
    "#behaves well with scikit-learn. Now let's take a look at what your\n",
    "#model found."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_coefficients(coefs, names, ax):\n",
    "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
    "    ax.bar(names, coefs)\n",
    "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
    "\n",
    "    # Set formatting so it looks nice\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    return ax\n",
    "\n",
    "#################################################\n",
    "\n",
    "# Visualize the output data up to \"2011-01\"\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-regression with a smoother time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the output data up to \"2011-01\"\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Cross-validating time series data**\n",
    "___\n",
    "- K-fold cross-validation is usually used in timeseries data\n",
    "- shuffling data in cross validation only works for data that is i.i.d.\n",
    "    - do not use when making predictions with a timeseries\n",
    "- using the time series CV iterator\n",
    "    - always use data from the **past** *(training data)* to predict the **future** *(test data)*\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation with shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import ShuffleSplit and create the cross-validation object\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "results = []\n",
    "for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "    model.fit(X[tr], y[tr])\n",
    "\n",
    "    # Generate predictions on the test data, score the predictions, and collect\n",
    "    prediction = model.predict(X[tt])\n",
    "    score = r2_score(y[tt], prediction)\n",
    "    results.append((prediction, score, tt))\n",
    "\n",
    "# Custom function to quickly visualize predictions\n",
    "visualize_predictions(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation without shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create KFold cross-validation object\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# Iterate through CV splits\n",
    "results = []\n",
    "for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "    model.fit(X[tr], y[tr])\n",
    "\n",
    "    # Generate predictions on the test data and collect\n",
    "    prediction = model.predict(X[tt])\n",
    "    results.append((prediction, tt))\n",
    "\n",
    "# Custom function to quickly visualize predictions\n",
    "visualize_predictions(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-based cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "fig, ax = plt.subplots()\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    ax.plot(tr, ii + y[tr])\n",
    "\n",
    "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Stationarity and stability**\n",
    "___\n",
    "- stationarity\n",
    "    - stationary timeseries do not change their statistical properties over time\n",
    "        -e.g., mean, standard deviation, trends\n",
    "    - most timeseries are non-stationary to some extent\n",
    "- stability\n",
    "    - non-stationary data results in variability in our model\n",
    "    - the statistical properties the model finds may change with the data\n",
    "    - we will be less certain about the correct values of model parameters\n",
    "- cross-validation to quantify parameter stability\n",
    "    - calculate model parameters on each iteration\n",
    "    - assess parameter stability across all CV splits\n",
    "- bootstrapping the mean\n",
    "    - a common way to assess variability\n",
    "    - take a random sample of data **with replacement**\n",
    "    - calculate the mean of the sample\n",
    "    - repeat this process thousands of times\n",
    "    - calculate the percentiles of the result (usually 2.5, 97.5)\n",
    "    - the result is a *95% confidence interval* of the mean of each coefficient\n",
    "- assessing model performance stability\n",
    "    - if using TimeSeriesSplit, we can *plot* the model's score over time\n",
    "    - this is useful in finding certain regions of time that hurt the score\n",
    "    - also useful to find non-stationary signals\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "B and C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping a confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
    "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
    "    # Create empty array to fill the results\n",
    "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
    "    for ii in range(n_boots):\n",
    "        # Generate random indices for data *with* replacement, then take the sample mean\n",
    "        random_sample = resample(data)\n",
    "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "\n",
    "    # Compute the percentiles of choice for the bootstrapped means\n",
    "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
    "    return percentiles\n",
    "\n",
    "#################################################\n",
    "#You can use this function to assess the variability of your model\n",
    "#coefficients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating variability in model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through CV splits\n",
    "n_splits = 100\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Create empty array to collect coefficients\n",
    "coefficients = np.zeros([n_splits, X.shape[1]])\n",
    "\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Fit the model on training data and collect the coefficients\n",
    "    model.fit(X[tr], y[tr])\n",
    "    coefficients[ii] = model.coef_\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Calculate a confidence interval around each coefficient\n",
    "bootstrapped_interval = bootstrap_interval(coefficients)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "ax.set(title='95% confidence interval for model coefficients')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing model score variability over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Generate scores for each split to see how the model performs over time\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "\n",
    "# Convert to a Pandas Series object\n",
    "scores_series = pd.Series(scores, index=times_scores, name='score')\n",
    "\n",
    "# Bootstrap a rolling confidence interval for the mean score\n",
    "scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\n",
    "scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
    "scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for non-stationarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-initialize window sizes\n",
    "window_sizes = [25, 50, 75, 100]\n",
    "\n",
    "# Create an empty DataFrame to collect the stores\n",
    "all_scores = pd.DataFrame(index=times_scores)\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\n",
    "for window in window_sizes:\n",
    "    # Create cross-validation object using a limited lookback window\n",
    "    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
    "\n",
    "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
    "    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "    all_scores['Length {}'.format(window)] = this_scores\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Visualize the scores\n",
    "ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
    "ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Wrap-up**\n",
    "___\n",
    "- Timeseries and machine learning\n",
    "    - the many applications of time series + machine learning\n",
    "    - always visualize the data first\n",
    "    - the scikit-learn API standardizes this process\n",
    "- Feature extraction and classification\n",
    "    - summary statistics for time series classification\n",
    "    - combining multiple features into a single input matrix\n",
    "    - feature extraction for time series data\n",
    "- Model fitting and improving data quality\n",
    "    - time series features for regression\n",
    "    - generating predictions over time\n",
    "    - cleaning and improving time series data\n",
    "- Validating and assessing our model performance\n",
    "    - cross-validation with time series data (don't shuffle the data!)\n",
    "    - time series stationarity\n",
    "    - assessing model coefficient and score stability\n",
    "- Advanced concepts in time series\n",
    "    - advanced window functions\n",
    "    - signal processing and filtering details\n",
    "    - spectral analysis\n",
    "- Advanced machine learning\n",
    "    - advanced time series feature extraction\n",
    "        - e.g., tsfresh\n",
    "    - more complex model architectures for regression and classification\n",
    "    - production-ready pipelines for time series analysis\n",
    "- Ways to practice\n",
    "    - Kaggle\n",
    "    - Quantopian\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
