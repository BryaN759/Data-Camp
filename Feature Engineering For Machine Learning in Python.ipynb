{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Why generate features?**\n",
    "___\n",
    "- Different types of data\n",
    "    - continuous: integers or floats\n",
    "    - categorical: one of a limited set of values\n",
    "    - ordinal: ranked values\n",
    "    - boolean: true/false values\n",
    "    - datetime: dates and times\n",
    "- course structure\n",
    "    - chapter 1: feature creation and extraction\n",
    "    - chapter 2: engineering messy data\n",
    "    - chapter 3: feature normalization\n",
    "    - chapter 4: working with text features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Getting to know your data\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import so_survey_csv into so_survey_df\n",
    "so_survey_df = pd.read_csv(so_survey_csv)\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "print(so_survey_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#          SurveyDate                                    FormalEducation  ConvertedSalary Hobby       Country  ...     VersionControl Age  Years Experience  Gender   RawSalary\n",
    "#    0  2/28/18 20:20           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN   Yes  South Africa  ...                Git  21                13    Male         NaN\n",
    "#    1  6/28/18 13:26           Bachelor's degree (BA. BS. B.Eng.. etc.)          70841.0   Yes       Sweeden  ...     Git;Subversion  38                 9    Male   70,841.00\n",
    "#    2    6/6/18 3:37           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN    No       Sweeden  ...                Git  45                11     NaN         NaN\n",
    "#    3    5/9/18 1:06  Some college/university study without earning ...          21426.0   Yes       Sweeden  ...  Zip file back-ups  46                12    Male   21,426.00\n",
    "#    4  4/12/18 22:41           Bachelor's degree (BA. BS. B.Eng.. etc.)          41671.0   Yes            UK  ...                Git  39                 7    Male  Â£41,671.00\n",
    "#\n",
    "#    [5 rows x 11 columns]\n",
    "#################################################\n",
    "\n",
    "# Print the data type of each column\n",
    "print(so_survey_df.dtypes)\n",
    "\n",
    "#################################################\n",
    "#    SurveyDate                     object\n",
    "#    FormalEducation                object\n",
    "#    ConvertedSalary               float64\n",
    "#    Hobby                          object\n",
    "#    Country                        object\n",
    "#    StackOverflowJobsRecommend    float64\n",
    "#    VersionControl                 object\n",
    "#    Age                             int64\n",
    "#    Years Experience                int64\n",
    "#    Gender                         object\n",
    "#    RawSalary                      object\n",
    "#    dtype: object\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Selecting specific data types\n",
    "\n",
    "# Create subset of only the numeric columns\n",
    "so_numeric_df = so_survey_df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Print the column names contained in so_survey_df_num\n",
    "print(so_numeric_df.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['ConvertedSalary', 'StackOverflowJobsRecommend', 'Age', 'Years Experience'], dtype='object')\n",
    "#################################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Dealing with categorical features**\n",
    "___\n",
    "- encoding categorical features\n",
    "    - one-hot encoding\n",
    "        - converts n categories into n features\n",
    "        - explainable features\n",
    "        - problem of collinearity\n",
    "    - dummy encoding\n",
    "        - converts n categories into n-1 features\n",
    "        -\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#One-hot encoding and dummy variables\n",
    "\n",
    "# Convert the Country column to a one hot encoded Data Frame\n",
    "one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
    "\n",
    "# Print the columns names\n",
    "print(one_hot_encoded.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend', 'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'OH_France', 'OH_India',\n",
    "#           'OH_Ireland', 'OH_Russia', 'OH_South Africa', 'OH_Spain', 'OH_Sweeden', 'OH_UK', 'OH_USA', 'OH_Ukraine'],\n",
    "#          dtype='object')\n",
    "#################################################\n",
    "\n",
    "# Create dummy variables for the Country column\n",
    "dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
    "\n",
    "# Print the columns names\n",
    "print(dummy.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend', 'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'DM_India', 'DM_Ireland',\n",
    "#           'DM_Russia', 'DM_South Africa', 'DM_Spain', 'DM_Sweeden', 'DM_UK', 'DM_USA', 'DM_Ukraine'],\n",
    "#          dtype='object')\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dealing with uncommon categories\n",
    "\n",
    "# Create a series out of the Country column\n",
    "countries = so_survey_df['Country']\n",
    "\n",
    "# Get the counts of each category\n",
    "country_counts = countries.value_counts()\n",
    "\n",
    "# Print the count values for each category\n",
    "print(country_counts)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    South Africa    166\n",
    "#    USA             164\n",
    "#    Spain           134\n",
    "#    Sweeden         119\n",
    "#    France          115\n",
    "#    Russia           97\n",
    "#    India            95\n",
    "#    UK               95\n",
    "#    Ukraine           9\n",
    "#    Ireland           5\n",
    "#    Name: Country, dtype: int64\n",
    "#################################################\n",
    "\n",
    "# Create a mask for only categories that occur less than 10 times\n",
    "mask = countries.isin(country_counts[country_counts < 10].index)\n",
    "\n",
    "# Print the top 5 rows in the mask series\n",
    "print(mask.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    False\n",
    "#    1    False\n",
    "#    2    False\n",
    "#    3    False\n",
    "#    4    False\n",
    "#    Name: Country, dtype: bool\n",
    "#################################################\n",
    "\n",
    "# Label all other categories as Other\n",
    "countries[mask] = 'Other'\n",
    "\n",
    "# Print the updated category counts\n",
    "print(pd.value_counts(countries))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    South Africa    166\n",
    "#    USA             164\n",
    "#    Spain           134\n",
    "#    Sweeden         119\n",
    "#    France          115\n",
    "#    Russia           97\n",
    "#    India            95\n",
    "#    UK               95\n",
    "#    Other            14\n",
    "#    Name: Country, dtype: int64\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Numeric variables**\n",
    "___\n",
    "- Types of numeric features\n",
    "    - age\n",
    "    - price\n",
    "    - counts\n",
    "    - geospatial data\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Binarizing columns\n",
    "\n",
    "# Create the Paid_Job column filled with zeros\n",
    "so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
    "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Paid_Job  ConvertedSalary\n",
    "#    0         0              0.0\n",
    "#    1         1          70841.0\n",
    "#    2         0              0.0\n",
    "#    3         1          21426.0\n",
    "#    4         1          41671.0\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Binning values\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary into 5 bins\n",
    "so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 5)\n",
    "\n",
    "# Print the first 5 rows of the equal_binned column\n",
    "print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#              equal_binned  ConvertedSalary\n",
    "#    0  (-2000.0, 400000.0]              0.0\n",
    "#    1  (-2000.0, 400000.0]          70841.0\n",
    "#    2  (-2000.0, 400000.0]              0.0\n",
    "#    3  (-2000.0, 400000.0]          21426.0\n",
    "#    4  (-2000.0, 400000.0]          41671.0\n",
    "#################################################\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Specify the boundaries of the bins\n",
    "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary using these boundaries\n",
    "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'],\n",
    "                                         bins, labels = labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      boundary_binned  ConvertedSalary\n",
    "#    0        Very low              0.0\n",
    "#    1          Medium          70841.0\n",
    "#    2        Very low              0.0\n",
    "#    3             Low          21426.0\n",
    "#    4             Low          41671.0\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Why do missing values exist?**\n",
    "___\n",
    "- How gaps in data occur\n",
    "    - data not being collected properly\n",
    "    - collection and management errors\n",
    "    - data intentionally being omitted\n",
    "    - could be created due to transformations of the data\n",
    "- Why we care?\n",
    "    - some models cannot work with missing data (Nulls/NaNs)\n",
    "    - missing data may be a sign of a wider data issue\n",
    "    - missing data can be a useful feature\n",
    "- pd.info()\n",
    "- pd.isnull()\n",
    "- pd.isnull().sum()\n",
    "- df.notnull()\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#How sparse is my data?\n",
    "\n",
    "# Subset the DataFrame\n",
    "sub_df = so_survey_df[['Age', 'Gender']]\n",
    "\n",
    "# Print the number of non-missing values\n",
    "print(sub_df.info())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    <class 'pandas.core.frame.DataFrame'>\n",
    "#    RangeIndex: 999 entries, 0 to 998\n",
    "#    Data columns (total 2 columns):\n",
    "#    Age       999 non-null int64\n",
    "#    Gender    693 non-null object\n",
    "#    dtypes: int64(1), object(1)\n",
    "#    memory usage: 15.7+ KB\n",
    "#    None\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Finding the missing values\n",
    "\n",
    "# Print the top 10 entries of the DataFrame\n",
    "print(sub_df.head(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Age  Gender\n",
    "#    0   21    Male\n",
    "#    1   38    Male\n",
    "#    2   45     NaN\n",
    "#    3   46    Male\n",
    "#    4   39    Male\n",
    "#    5   39    Male\n",
    "#    6   34    Male\n",
    "#    7   24  Female\n",
    "#    8   23    Male\n",
    "#    9   36     NaN\n",
    "#################################################\n",
    "\n",
    "# Print the locations of the missing values\n",
    "print(sub_df.head(10).isnull())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#         Age  Gender\n",
    "#    0  False   False\n",
    "#    1  False   False\n",
    "#    2  False    True\n",
    "#    3  False   False\n",
    "#    4  False   False\n",
    "#    5  False   False\n",
    "#    6  False   False\n",
    "#    7  False   False\n",
    "#    8  False   False\n",
    "#    9  False    True\n",
    "#################################################\n",
    "\n",
    "# Print the locations of the non-missing values\n",
    "print(sub_df.head(10).notnull())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        Age  Gender\n",
    "#    0  True    True\n",
    "#    1  True    True\n",
    "#    2  True   False\n",
    "#    3  True    True\n",
    "#    4  True    True\n",
    "#    5  True    True\n",
    "#    6  True    True\n",
    "#    7  True    True\n",
    "#    8  True    True\n",
    "#    9  True   False\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Dealing with missing values (I)**\n",
    "___\n",
    "- pd.dropna()\n",
    "- pd.drop()\n",
    "- random omissions\n",
    "    - complete case analysis / listwise deletion\n",
    "    - drawbacks\n",
    "        - deletes valid data points as well\n",
    "        - relies on randomness\n",
    "        - reduces information if a feature is removed (degrees of freedom)\n",
    "- replacement\n",
    "- recording missing values\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Listwise deletion\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(so_survey_df.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (999, 11)\n",
    "#################################################\n",
    "\n",
    "# Create a new DataFrame dropping all incomplete rows\n",
    "no_missing_values_rows = so_survey_df.dropna()\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(no_missing_values_rows.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (264, 11)\n",
    "#################################################\n",
    "\n",
    "# Create a new DataFrame dropping all columns with incomplete rows\n",
    "no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(no_missing_values_cols.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (999, 7)\n",
    "#################################################\n",
    "\n",
    "# Drop all rows where Gender is missing\n",
    "no_gender = so_survey_df.dropna(subset=['Gender'])\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(no_gender.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (693, 11)\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Replacing missing values with constants\n",
    "\n",
    "# Print the count of occurrences\n",
    "print(so_survey_df['Gender'].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Male                                                                        632\n",
    "#    Female                                                                 53\n",
    "#    Female;Male                                                                 2\n",
    "#    Transgender                                                               2\n",
    "#    Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming      1\n",
    "#    Female;Transgender                                                             1\n",
    "#    Male;Non-binary. genderqueer. or gender non-conforming                         1\n",
    "#    Non-binary. genderqueer. or gender non-conforming                              1\n",
    "#    Name: Gender, dtype: int64\n",
    "#################################################\n",
    "\n",
    "# Replace missing values\n",
    "so_survey_df['Gender'].fillna(value='Not Given', inplace=True)\n",
    "\n",
    "# Print the count of each value\n",
    "print(so_survey_df['Gender'].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Male                                                                         632\n",
    "#    Not Given                                                                    306\n",
    "#    Female                                                                        53\n",
    "#    Female;Male                                                                    2\n",
    "#    Transgender                                                                    2\n",
    "#    Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming      1\n",
    "#    Female;Transgender                                                             1\n",
    "#    Male;Non-binary. genderqueer. or gender non-conforming                         1\n",
    "#    Non-binary. genderqueer. or gender non-conforming                              1\n",
    "#    Name: Gender, dtype: int64\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Dealing with missing values (II)**\n",
    "___\n",
    "- If you cannot drop rows, what else can you do?\n",
    "    - **Categorical columns**: replace missing values with the most common occurring value or with a string that flags missing values such as 'None'\n",
    "    - **Numeric columns**: replace missing values with a suitable value\n",
    "        - measure of central tendency, e.g., mean, median\n",
    "- impute values based on the train set to both train and test sets.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Filling continuous missing values\n",
    "\n",
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    NaN\n",
    "#    1    7.0\n",
    "#    2    8.0\n",
    "#    3    NaN\n",
    "#    4    8.0\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "\n",
    "# Fill missing values with the mean\n",
    "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
    "\n",
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    7.061602\n",
    "#    1    7.000000\n",
    "#    2    8.000000\n",
    "#    3    7.061602\n",
    "#    4    8.000000\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "\n",
    "# Round the StackOverflowJobsRecommend values\n",
    "so_survey_df['StackOverflowJobsRecommend'] = np.round (so_survey_df['StackOverflowJobsRecommend'])\n",
    "\n",
    "# Print the top 5 rows\n",
    "print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    7.0\n",
    "#    1    7.0\n",
    "#    2    8.0\n",
    "#    3    7.0\n",
    "#    4    8.0\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Dealing with other data issues**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dealing with stray characters (I)\n",
    "\n",
    "# Remove the commas in the column\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',', '')\n",
    "\n",
    "# Remove the dollar signs in the column\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('$', '')\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dealing with stray characters (II)\n",
    "\n",
    "# Attempt to convert the column to numeric values\n",
    "numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
    "\n",
    "# Find the indexes of missing values\n",
    "idx = numeric_vals.isna()\n",
    "\n",
    "# Print the relevant rows\n",
    "print(so_survey_df['RawSalary'][idx])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0             NaN\n",
    "#    2             NaN\n",
    "#    4       Â£41671.00\n",
    "#    6             NaN\n",
    "#    8             NaN\n",
    "#    ...\n",
    "#    49      Â£19500.00\n",
    "#    50            NaN\n",
    "#    52            NaN\n",
    "#    53      Â£36000.00\n",
    "#    54            NaN\n",
    "#    Name: RawSalary, Length: 401, dtype: object\n",
    "#################################################\n",
    "\n",
    "# Replace the offending characters\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('Â£', '')\n",
    "\n",
    "# Convert the column to float\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].astype('float')\n",
    "\n",
    "# Print the column\n",
    "print(so_survey_df['RawSalary'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0            NaN\n",
    "#    1        70841.0\n",
    "#    2            NaN\n",
    "#    3        21426.0\n",
    "#    4        41671.0\n",
    "#    ...\n",
    "#    994          NaN\n",
    "#    995      58746.0\n",
    "#    996      55000.0\n",
    "#    997          NaN\n",
    "#    998    1000000.0\n",
    "#    Name: RawSalary, Length: 999, dtype: float64\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Method chaining\n",
    "\n",
    "# Method chaining\n",
    "df['column'] = df['column'].method1().method2().method3()\n",
    "\n",
    "# Same as\n",
    "df['column'] = df['column'].method1()\n",
    "df['column'] = df['column'].method2()\n",
    "df['column'] = df['column'].method3()\n",
    "\n",
    "#In this exercise you will repeat the steps you performed in the last\n",
    "#two exercises, but do so using method chaining.\n",
    "\n",
    "# Use method chaining\n",
    "so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
    "                              .str.replace(',', '')\\\n",
    "                              .str.replace('$', '')\\\n",
    "                              .str.replace('Â£', '')\\\n",
    "                              .astype('float')\n",
    "\n",
    "# Print the RawSalary column\n",
    "print(so_survey_df['RawSalary'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0            NaN\n",
    "#    1        70841.0\n",
    "#    2            NaN\n",
    "#    3        21426.0\n",
    "#    4        41671.0\n",
    "#    ...\n",
    "#    994          NaN\n",
    "#    995      58746.0\n",
    "#    996      55000.0\n",
    "#    997          NaN\n",
    "#    998    1000000.0\n",
    "#    Name: RawSalary, Length: 999, dtype: float64\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Data distributions**\n",
    "___\n",
    "- most models assume your data is normally distributed and/or on the same scale\n",
    "    - 1 sd = 66.27%; 2 sd = 95.45%; 3 sd = 99.73%\n",
    "- decision tree-based models do not make this assumption\n",
    "    - As decision trees split along a singular point, they do not require all the columns to be on the same scale.\n",
    "- delving deeper with box plots\n",
    "    - Interquartile Range (IQR) = 25th (Q1) percentile to 75th (Q3) percentile\n",
    "    - Minimum = Q1 - 1.5 IQR\n",
    "    - Maximum = Q3 + 1.5 IQR\n",
    "    - outliers are outside Minimum or Maximum\n",
    "- pairing distributions with seaborn library\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#What does your data look like? (I)\n",
    "\n",
    "# Create a histogram\n",
    "so_numeric_df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a boxplot of two columns\n",
    "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a boxplot of ConvertedSalary\n",
    "so_numeric_df[['ConvertedSalary']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#What does your data look like? (II)\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot pairwise relationships\n",
    "sns.pairplot(so_numeric_df)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print(so_numeric_df.describe())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#           ConvertedSalary         Age  Years Experience\n",
    "#    count     9.990000e+02  999.000000        999.000000\n",
    "#    mean      6.161746e+04   36.003003          9.961962\n",
    "#    std       1.760924e+05   13.255127          4.878129\n",
    "#    min       0.000000e+00   18.000000          0.000000\n",
    "#    25%       0.000000e+00   25.000000          7.000000\n",
    "#    50%       2.712000e+04   35.000000         10.000000\n",
    "#    75%       7.000000e+04   45.000000         13.000000\n",
    "#    max       2.000000e+06   83.000000         27.000000\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Scaling and transformations**\n",
    "___\n",
    "- Min-Max scaling / Normalization\n",
    "    - distribution remains the same\n",
    "    - values change to range 0-1\n",
    "    - MinMaxScaler() from scikit-learn preprocessing module\n",
    "- Standardization\n",
    "    - centers distribution around the mean = zero\n",
    "    - StandardScaler() from scikit-learn preprocessing module\n",
    "- Log Transformation\n",
    "    - can make highly skewed distributions less skewed\n",
    "    - PowerTransformer() from scikit-learn preprocessing module\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "# Import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "MM_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit MM_scaler to the data\n",
    "MM_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "print(so_numeric_df[['Age_MM', 'Age']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#         Age_MM  Age\n",
    "#    0  0.046154   21\n",
    "#    1  0.307692   38\n",
    "#    2  0.415385   45\n",
    "#    3  0.430769   46\n",
    "#    4  0.323077   39\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Standardization\n",
    "\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit SS_scaler to the data\n",
    "SS_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "print(so_numeric_df[['Age_SS', 'Age']].head())\n",
    "\n",
    "#################################################\n",
    "#       Age_SS  Age\n",
    "#    0 -1.132431   21\n",
    "#    1  0.150734   38\n",
    "#    2  0.679096   45\n",
    "#    3  0.754576   46\n",
    "#    4  0.226214   39\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Log transformation\n",
    "\n",
    "# Import PowerTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Instantiate PowerTransformer\n",
    "pow_trans = PowerTransformer()\n",
    "\n",
    "# Train the transform on the data\n",
    "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Apply the power transform to the data\n",
    "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Plot the data before and after the transformation\n",
    "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![_images/16.5.svg](_images/16.5.svg)\n",
    "Did you notice the change in the shape of the distribution?\n",
    "ConvertedSalary_LG column looks much more normal than the original\n",
    "ConvertedSalary column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Removing outliers**\n",
    "___\n",
    "- Quantile based detection\n",
    "- Standard deviation based detection\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Percentage based outlier removal\n",
    "\n",
    "\n",
    "# Find the 95th quantile\n",
    "quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
    "\n",
    "# Trim the outliers\n",
    "trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
    "\n",
    "# The original histogram\n",
    "so_numeric_df[['ConvertedSalary']].hist()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# The trimmed histogram\n",
    "trimmed_df[['ConvertedSalary']].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Find the mean and standard dev\n",
    "std = so_numeric_df['ConvertedSalary'].std()\n",
    "mean = so_numeric_df['ConvertedSalary'].mean()\n",
    "\n",
    "# Calculate the cutoff\n",
    "cut_off = std * 3\n",
    "lower, upper = mean - cut_off, mean + cut_off\n",
    "\n",
    "# Trim the outliers\n",
    "trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper) \\\n",
    "                           & (so_numeric_df['ConvertedSalary'] > lower)]\n",
    "\n",
    "# The trimmed box plot\n",
    "trimmed_df[['ConvertedSalary']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train and testing transformations (I)\n",
    "\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply a standard scaler to the data\n",
    "SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit the standard scaler to the data\n",
    "SS_scaler.fit(so_train_numeric[['Age']])\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
    "print(so_test_numeric[['Age', 'Age_ss']].head())\n",
    "\n",
    "#################################################\n",
    "#       Age    Age_ss\n",
    "#    700   35 -0.069265\n",
    "#    701   18 -1.343218\n",
    "#    702   47  0.829997\n",
    "#    703   57  1.579381\n",
    "#    704   41  0.380366\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train and testing transformations (II)\n",
    "\n",
    "#Similar to the last exercise, we split the so_numeric_df DataFrame\n",
    "#into train (so_train_numeric) and test (so_test_numeric) sets.\n",
    "\n",
    "train_std = so_train_numeric['ConvertedSalary'].std()\n",
    "train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
    "\n",
    "cut_off = train_std * 3\n",
    "train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
    "\n",
    "# Trim the test DataFrame\n",
    "trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \\\n",
    "                             & (so_test_numeric['ConvertedSalary'] > train_lower)]\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Encoding text**\n",
    "___\n",
    "- Standardizing your text\n",
    "    - remove unwanted characters using str.replace() and regular expressions\n",
    "        - [a-zA-Z]: all letter characters\n",
    "        - [^a-zA-Z]: all non letter characters\n",
    "- Standardize the case\n",
    "    - str.lower()\n",
    "- length of text\n",
    "    - .len()\n",
    "- word count\n",
    "- average length of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning up your text\n",
    "\n",
    "# Print the first 5 rows of the text column\n",
    "print(speech_df['text'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    Fellow-Citizens of the Senate and of the House...\n",
    "#    1    Fellow Citizens:  I AM again called upon by th...\n",
    "#    2    WHEN it was first perceived, in early times, t...\n",
    "#    3    Friends and Fellow-Citizens:  CALLED upon to u...\n",
    "#    4    PROCEEDING, fellow-citizens, to that qualifica...\n",
    "#    Name: text, dtype: object\n",
    "#################################################\n",
    "\n",
    "# Replace all non letter characters with a whitespace\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "# Change to lower case\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "print(speech_df['text_clean'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    fellow citizens of the senate and of the house...\n",
    "#    1    fellow citizens   i am again called upon by th...\n",
    "#    2    when it was first perceived  in early times  t...\n",
    "#    3    friends and fellow citizens   called upon to u...\n",
    "#    4    proceeding  fellow citizens  to that qualifica...\n",
    "#    Name: text_clean, dtype: object\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#High level text features\n",
    "\n",
    "# Find the length of each text\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                                               text_clean  char_cnt  word_cnt  avg_word_length\n",
    "#    0   fellow citizens of the senate and of the house...      8616      1432         6.016760\n",
    "#    1   fellow citizens   i am again called upon by th...       787       135         5.829630\n",
    "#    2   when it was first perceived  in early times  t...     13871      2323         5.971158\n",
    "#    3   friends and fellow citizens   called upon to u...     10144      1736         5.843318\n",
    "#    4   proceeding  fellow citizens  to that qualifica...     12902      2169         5.948363\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Word counts**\n",
    "___\n",
    "- text to columns\n",
    "    - one column per word with word counts for each word\n",
    "    - CountVectorizer in sklearn.feature_extraction.text\n",
    "        - min_df, max_df = 0.1, 0.9\n",
    "        - creates sparse array, convert to array using .toarray()\n",
    "            - to get feature names from the array .get_feature_names()\n",
    "        - combine into dataframe\n",
    "            - pd.DataFrame(cv_tranformed.toarray(), columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "        - updating/combining your dataframe\n",
    "            - pd.concat([list], axis=1, sort=False)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Counting words (I)\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    ['abandon', 'abandoned', 'abandonment', 'abate', 'abdicated', 'abeyance', 'abhorring', 'abide', 'abiding', 'abilities', 'ability', 'abject', 'able', 'ably', 'abnormal', 'abode', 'abolish', 'abolished', 'abolishing', 'aboriginal', 'aborigines', 'abound', 'abounding', 'abounds', 'about', 'above', 'abraham', 'abreast', 'abridging',\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Counting words (II)\n",
    "\n",
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[0 0 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]\n",
    "#     [0 1 0 ... 0 0 0]\n",
    "#     ...\n",
    "#     [0 1 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]]\n",
    "#################################################\n",
    "\n",
    "# Print the shape of cv_array\n",
    "print(cv_array.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (58, 9043)\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Limiting your features\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Specify arguments to limit the number of features generated\n",
    "cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "print(cv_array.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (58, 818)\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Text to DataFrame\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "cv_df = pd.DataFrame(cv_array,\n",
    "                     columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "print(speech_df_new.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    Name         Inaugural Address                      Date                                               text                                         text_clean  ...  Counts_years  \\\n",
    "#    0  George Washington   First Inaugural Address  Thursday, April 30, 1789  Fellow-Citizens of the Senate and of the House...  fellow citizens of the senate and of the house...  ...             1\n",
    "#    1  George Washington  Second Inaugural Address     Monday, March 4, 1793  Fellow Citizens:  I AM again called upon by th...  fellow citizens   i am again called upon by th...  ...             0\n",
    "#    2         John Adams         Inaugural Address   Saturday, March 4, 1797  WHEN it was first perceived, in early times, t...  when it was first perceived  in early times  t...  ...             3\n",
    "#    3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801  Friends and Fellow-Citizens:  CALLED upon to u...  friends and fellow citizens   called upon to u...  ...             0\n",
    "#    4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805  PROCEEDING, fellow-citizens, to that qualifica...  proceeding  fellow citizens  to that qualifica...  ...             2\n",
    "#\n",
    "#       Counts_yet  Counts_you  Counts_young  Counts_your\n",
    "#    0           0           5             0            9\n",
    "#    1           0           0             0            1\n",
    "#    2           0           0             0            1\n",
    "#    3           2           7             0            7\n",
    "#    4           2           4             0            4\n",
    "#\n",
    "#    [5 rows x 826 columns]\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Term frequency-inverse document frequency**\n",
    "___\n",
    "- TF-IDF =\n",
    "    - count of word occurances / Total words in document\n",
    "        - Divided by\n",
    "    - log (Number of docs word is in / Total number of documents)\n",
    "- reduces the weight of common words and increases weights of uncommon words\n",
    "- from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    - max_features - maximum number of columns\n",
    "    - stop_words - list of common words to omit\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Tf-idf\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the data\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(),\n",
    "                    columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "print(tv_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  TFIDF_americans  ...  TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years\n",
    "#    0      0.000000              0.133415       0.000000        0.105388              0.0  ...   0.000000   0.060755    0.000000     0.045929     0.052694\n",
    "#    1      0.000000              0.261016       0.266097        0.000000              0.0  ...   0.000000   0.000000    0.000000     0.000000     0.000000\n",
    "#    2      0.000000              0.092436       0.157058        0.073018              0.0  ...   0.024339   0.000000    0.000000     0.063643     0.073018\n",
    "#    3      0.000000              0.092693       0.000000        0.000000              0.0  ...   0.036610   0.000000    0.039277     0.095729     0.000000\n",
    "#    4      0.041334              0.039761       0.000000        0.031408              0.0  ...   0.094225   0.000000    0.000000     0.054752     0.062817\n",
    "#\n",
    "#    [5 rows x 100 columns]\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Inspecting Tf-idf values\n",
    "\n",
    "# Isolate the row to be examined\n",
    "sample_row = tv_df.iloc[0]\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "print(sample_row.sort_values(ascending=False).head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    TFIDF_government    0.367430\n",
    "#    TFIDF_public        0.333237\n",
    "#    TFIDF_present       0.315182\n",
    "#    TFIDF_duty          0.238637\n",
    "#    TFIDF_citizens      0.229644\n",
    "#    Name: 0, dtype: float64\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Transforming unseen data\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the data\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(),\n",
    "                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "print(test_tv_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  TFIDF_authority  ...  TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years\n",
    "#    0      0.000000              0.029540       0.233954        0.082703         0.000000  ...   0.079050   0.033313    0.000000     0.299983     0.134749\n",
    "#    1      0.000000              0.000000       0.547457        0.036862         0.000000  ...   0.052851   0.066817    0.078999     0.277701     0.126126\n",
    "#    2      0.000000              0.000000       0.126987        0.134669         0.000000  ...   0.042907   0.054245    0.096203     0.225452     0.043884\n",
    "#    3      0.037094              0.067428       0.267012        0.031463         0.039990  ...   0.030073   0.038020    0.235998     0.237026     0.061516\n",
    "#    4      0.000000              0.000000       0.221561        0.156644         0.028442  ...   0.021389   0.081124    0.119894     0.299701     0.153133\n",
    "#\n",
    "#    [5 rows x 100 columns]\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**N-grams**\n",
    "___\n",
    "- Bag of words\n",
    "    - words viewed/analyzed independently\n",
    "    - valence (positive/negative) is ignored\n",
    "- ngram_range\n",
    "    -argument in TfidfVectorizer\n",
    "    - indicates bigrams, trigrams, etc for more context to be considered\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Using longer n-grams\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate a trigram vectorizer\n",
    "cv_trigram_vec = CountVectorizer(max_features=100,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range = (3,3))\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "print(cv_trigram_vec.get_feature_names())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "# ['ability preserve protect', 'agriculture commerce manufactures',\n",
    "# 'america ideal freedom', 'amity mutual concession', 'anchor peace home',\n",
    "# 'ask bow heads', 'best ability preserve', 'best interests country',\n",
    "# 'bless god bless', 'bless united states', 'chief justice mr',\n",
    "# 'children children children', 'citizens united states',\n",
    "# 'civil religious liberty', 'civil service reform', 'commerce united states',\n",
    "# 'confidence fellow citizens', 'congress extraordinary session', 'constitution does expressly', 'constitution united states', 'coordinate branches government', 'day task people', 'defend constitution united', 'distinction powers granted', 'distinguished guests fellow', 'does expressly say', 'equal exact justice', 'era good feeling', 'executive branch government', 'faithfully execute office', 'fellow citizens assembled', 'fellow citizens called', 'fellow citizens large', 'fellow citizens world', 'form perfect union', 'general welfare secure', 'god bless america', 'god bless god', 'good greatest number', 'government peace war', 'government united states', 'granted federal government', 'great body people', 'great political parties', 'greatest good greatest', 'guests fellow citizens', 'invasion wars powers', 'land new promise', 'laws faithfully executed', 'letter spirit constitution', 'liberty pursuit happiness', 'life liberty pursuit', 'local self government', 'make hard choices', 'men women children', 'mr chief justice', 'mr majority leader', 'mr president vice', 'mr speaker mr', 'mr vice president', 'nation like person', 'new breeze blowing', 'new states admitted', 'north south east', 'oath prescribed constitution', 'office president united', 'passed generation generation', 'peace shall strive', 'people united states', 'physical moral political', 'policy united states', 'power general government', 'preservation general government', 'preservation sacred liberty', 'preserve protect defend', 'president united states', 'president vice president', 'promote general welfare', 'proof confidence fellow', 'protect defend constitution', 'protection great interests', 'reform civil service', 'reserved states people', 'respect individual human', 'right self government', 'secure blessings liberty', 'south east west', 'sovereignty general government', 'states admitted union', 'territories united states', 'thank god bless', 'turning away old', 'united states america', 'united states best', 'united states government', 'united states great', 'united states maintain', 'united states territory', 'vice president mr', 'welfare secure blessings']\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Finding the most common words\n",
    "\n",
    "# Create a DataFrame of the features\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(),\n",
    "                         columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted output\n",
    "print(cv_tri_df.sum().sort_values(ascending=False).head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Counts_constitution united states    20\n",
    "#    Counts_people united states          13\n",
    "#    Counts_preserve protect defend       10\n",
    "#    Counts_mr chief justice              10\n",
    "#    Counts_president united states        8\n",
    "#    dtype: int64\n",
    "#################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Wrap-up**\n",
    "___\n",
    "- Chapter 1\n",
    "    - how to understand your data types\n",
    "    - efficient encoding of categorical features\n",
    "    - different ways to work with continuous variables\n",
    "- Chapter 2\n",
    "    - how to locate gaps in your data\n",
    "    - best practices in dealing with incomplete rows\n",
    "    - methods to find and deal with unwanted characters\n",
    "- Chapter 3\n",
    "    - how to observe your data's distribution\n",
    "    - why and how to modify this distribution\n",
    "    - best practices of finding outliers and their removal\n",
    "- Chapter 4\n",
    "    - the foundations of word embeddings\n",
    "    - usage of Term Frequency Inverse Document Frequency (Tf-idf)\n",
    "    - n-grams and its advantages over bag of words\n",
    "- Next steps\n",
    "    - Kaggle competitions\n",
    "    - more DataCamp courses\n",
    "    - your own project\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
